# 라이브러리 불러오기
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 랜덤 시드 고정
import random
np.random.seed(1234)
random.seed(1234)

# 데이터셋 불러오기
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")
submission = pd.read_csv("sample_submission.csv")

# 경고 메시지 제거
import warnings
warnings.filterwarnings("ignore")

# 학습 데이터 확인
train_df.head()

# 데이터 타입 확인
train_df.dtypes

# 범주형 변수 예시 확인 (MSZoning)
train_df["MSZoning"].value_counts()

# 결측치 확인 (학습 데이터)
missing_counts = train_df.isnull().sum()
print(missing_counts[missing_counts > 0].sort_values(ascending=False))

# 결측치 확인 (테스트 데이터)
missing_counts = test_df.isnull().sum()
print(missing_counts[missing_counts > 0].sort_values(ascending=False))

# 레이블 인코딩을 위한 라이브러리 임포트
from sklearn.preprocessing import LabelEncoder

# 범주형 변수 추출
train_categories = train_df.columns[train_df.dtypes=="object"]
print(f"train_categories: {train_categories}")

test_categories = test_df.columns[test_df.dtypes=="object"]
print(f"test_categories: {test_categories}")

# 학습 데이터의 결측치 처리 및 레이블 인코딩
for cat in train_categories:
    le = LabelEncoder()
    print(cat)
    train_df[cat] = train_df[cat].fillna("missing")  # 결측치를 'missing' 문자열로 대체
    le = le.fit(train_df[cat])                      # 학습
    train_df[cat] = le.transform(train_df[cat])     # 숫자로 변환
    train_df[cat] = train_df[cat].astype("category")

# 테스트 데이터의 결측치 처리 및 레이블 인코딩
for cat in test_categories:
    le = LabelEncoder()
    print(cat)
    test_df[cat] = test_df[cat].fillna("missing")
    le = le.fit(test_df[cat])
    test_df[cat] = le.transform(test_df[cat])
    test_df[cat] = test_df[cat].astype("category")

# KFold 설정 (3-Fold 교차 검증)
from sklearn.model_selection import KFold
folds=3
kf = KFold(n_splits=folds)

# LightGBM 및 하이퍼파라미터 설정
import lightgbm as lgb
lgbm_params = {
    "objective":"regression",  # 회귀 문제
    "random_seed":1234
}

# 설명변수와 목표변수 분리
train_X = train_df.drop(["SalePrice", "Id"], axis=1)
train_Y = train_df["SalePrice"]

# 손실 함수: RMSE 계산용
from sklearn.metrics import mean_squared_error

# 조기 종료 콜백
from lightgbm import early_stopping

# 모델 학습 및 예측 저장 (로그 변환 전)
models = []
rmses = []
oof = np.zeros(len(train_X))  # out-of-fold 예측 저장

for train_index, val_index in kf.split(train_X):
    X_train = train_X.iloc[train_index]
    X_valid = train_X.iloc[val_index]
    y_train = train_Y.iloc[train_index]
    y_valid = train_Y.iloc[val_index]

    # LightGBM용 Dataset 생성
    lgb_train = lgb.Dataset(X_train, y_train)
    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)    
    
    # 모델 학습
    model_lgb = lgb.train(
        params=lgbm_params,
        train_set=lgb_train,
        valid_sets=[lgb_eval],
        num_boost_round=1000,
        callbacks=[early_stopping(stopping_rounds=100, verbose=True)]
    )

    # 예측 및 평가 (로그 변환 후 RMSE)
    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)
    tmp_rmse = np.sqrt(mean_squared_error(np.log(y_valid), np.log(y_pred)))
    print(tmp_rmse)    
              
    models.append(model_lgb)    
    rmses.append(tmp_rmse)
    oof[val_index] = y_pred

# 평균 RMSE 출력
sum(rmses)/len(rmses)

# 실제값과 예측값을 DataFrame으로 시각화
actual_pred_df = pd.DataFrame({
    'actual': train_Y,
    'pred': oof
})

# 산점도 그래프 (실제값 vs 예측값)
fig, ax = plt.subplots(figsize=(10, 8))
lims = [
    np.min([actual_pred_df['actual'].min(), actual_pred_df['pred'].min()]),
    np.max([actual_pred_df['actual'].max(), actual_pred_df['pred'].max()]),
]
ax.plot(lims, lims, 'r--', alpha=0.75, zorder=0, label='y = x (Perfect Prediction)')
ax.scatter(actual_pred_df['actual'], actual_pred_df['pred'], alpha=0.6, label='Actual vs. Predicted')
ax.set_xlabel('Actual Values', fontsize=12)
ax.set_ylabel('Predicted Values', fontsize=12)
ax.set_title('Actual vs. Predicted Values', fontsize=15)
ax.set_xlim(lims)
ax.set_ylim(lims)
ax.legend()
plt.grid(True)
plt.show()

# 중요 변수 시각화
for model in models:
    lgb.plot_importance(model, importance_type="gain", max_num_features=15)

# 목표변수 분포 확인
train_df["SalePrice"].describe()
train_df["SalePrice"].plot.hist(bins=20)

# 로그 변환 후 분포 확인
np.log(train_df['SalePrice']).plot.hist(bins=20)

# 로그 값 생성
train_df["SalePrice_log"] = np.log(train_df["SalePrice"])
train_X = train_df.drop(["SalePrice", "SalePrice_log", "Id"], axis=1)
train_Y = train_df["SalePrice_log"]

# 로그변환 후 다시 모델 학습 및 평가
models = []
rmses = []
oof = np.zeros(len(train_X))

for train_index, val_index in kf.split(train_X):
    X_train = train_X.iloc[train_index]
    X_valid = train_X.iloc[val_index]
    y_train = train_Y.iloc[train_index]
    y_valid = train_Y.iloc[val_index]

    lgb_train = lgb.Dataset(X_train, y_train)
    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)    
    
    model_lgb = lgb.train(
        params=lgbm_params,
        train_set=lgb_train,
        valid_sets=[lgb_eval],
        num_boost_round=1000,
        callbacks=[early_stopping(stopping_rounds=100, verbose=True)]
    )

    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)
    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))  # 로그값 그대로 RMSE 계산
    print(tmp_rmse)    
              
    models.append(model_lgb)    
    rmses.append(tmp_rmse)
    oof[val_index] = y_pred

# 평균 RMSE 확인
sum(rmses)/len(rmses)
